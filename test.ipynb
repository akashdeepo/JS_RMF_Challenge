{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]\n",
      "Pandas version: 1.5.3\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Display installed versions (optional for debugging)\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For handling large datasets and data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=0\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=1\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=2\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=3\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=4\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=5\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=6\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=7\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=8\\\\part-0.parquet', 'jane-street-real-time-market-data-forecasting\\\\train.parquet\\\\partition_id=9\\\\part-0.parquet']\n",
      "   date_id  time_id  symbol_id    weight  feature_00  feature_01  feature_02  \\\n",
      "0        0        0          1  3.889038         NaN         NaN         NaN   \n",
      "1        0        0          7  1.370613         NaN         NaN         NaN   \n",
      "2        0        0          9  2.285698         NaN         NaN         NaN   \n",
      "3        0        0         10  0.690606         NaN         NaN         NaN   \n",
      "4        0        0         14  0.440570         NaN         NaN         NaN   \n",
      "\n",
      "   feature_03  feature_04  feature_05  ...  feature_78  responder_0  \\\n",
      "0         NaN         NaN    0.851033  ...   -0.281498     0.738489   \n",
      "1         NaN         NaN    0.676961  ...   -0.302441     2.965889   \n",
      "2         NaN         NaN    1.056285  ...   -0.096792    -0.864488   \n",
      "3         NaN         NaN    1.139366  ...   -0.296244     0.408499   \n",
      "4         NaN         NaN    0.955200  ...    3.418133    -0.373387   \n",
      "\n",
      "   responder_1  responder_2  responder_3  responder_4  responder_5  \\\n",
      "0    -0.069556     1.380875     2.005353     0.186018     1.218368   \n",
      "1     1.190077    -0.523998     3.849921     2.626981     5.000000   \n",
      "2    -0.280303    -0.326697     0.375781     1.271291     0.099793   \n",
      "3     0.223992     2.294888     1.097444     1.225872     1.225376   \n",
      "4    -0.502764    -0.348021    -3.928148    -1.591366    -5.000000   \n",
      "\n",
      "   responder_6  responder_7  responder_8  \n",
      "0     0.775981     0.346999     0.095504  \n",
      "1     0.703665     0.216683     0.778639  \n",
      "2     2.109352     0.670881     0.772828  \n",
      "3     1.114137     0.775199    -1.379516  \n",
      "4    -3.572820    -1.089123    -5.000000  \n",
      "\n",
      "[5 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Corrected path using raw string to handle backslashes\n",
    "train_path = r\"jane-street-real-time-market-data-forecasting\\train.parquet\"\n",
    "\n",
    "# Get all partition files\n",
    "train_files = glob(os.path.join(train_path, 'partition_id=*', '*.parquet'))\n",
    "\n",
    "# Verify that files are detected\n",
    "print(train_files)\n",
    "\n",
    "# Load a sample partition (first one)\n",
    "if len(train_files) > 0:\n",
    "    sample_partition = pd.read_parquet(train_files[0])\n",
    "    print(sample_partition.head())\n",
    "else:\n",
    "    print(\"No files found. Please check the directory structure.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_00    1944210\n",
      "feature_01    1944210\n",
      "feature_02    1944210\n",
      "feature_03    1944210\n",
      "feature_04    1944210\n",
      "feature_08      16980\n",
      "feature_15      54992\n",
      "feature_16         63\n",
      "feature_17       9232\n",
      "feature_18         59\n",
      "feature_19         59\n",
      "feature_21    1944210\n",
      "feature_26    1944210\n",
      "feature_27    1944210\n",
      "feature_31    1944210\n",
      "feature_32      21737\n",
      "feature_33      21737\n",
      "feature_39     324732\n",
      "feature_40      38328\n",
      "feature_41      97113\n",
      "feature_42     324732\n",
      "feature_43      38328\n",
      "feature_44      97113\n",
      "feature_45     166374\n",
      "feature_46     166374\n",
      "feature_47         87\n",
      "feature_50     293120\n",
      "feature_51       2290\n",
      "feature_52      64120\n",
      "feature_53     293120\n",
      "feature_54       2290\n",
      "feature_55      64120\n",
      "feature_56         59\n",
      "feature_57         59\n",
      "feature_58      21732\n",
      "feature_62     153999\n",
      "feature_63     133274\n",
      "feature_64     136458\n",
      "feature_65     166374\n",
      "feature_66     166374\n",
      "feature_73      21732\n",
      "feature_74      21732\n",
      "feature_75         16\n",
      "feature_76         16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = sample_partition.isnull().sum()\n",
    "\n",
    "# Display the columns with missing values and their counts\n",
    "print(missing_values[missing_values > 0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            date_id       time_id     symbol_id        weight  feature_00  \\\n",
      "count  1.944210e+06  1.944210e+06  1.944210e+06  1.944210e+06         0.0   \n",
      "mean   9.384629e+01  4.240000e+02  1.376638e+01  1.973281e+00         NaN   \n",
      "std    4.813196e+01  2.450851e+02  1.108778e+01  9.679003e-01         NaN   \n",
      "min    0.000000e+00  0.000000e+00  0.000000e+00  4.405696e-01         NaN   \n",
      "25%    5.400000e+01  2.120000e+02  7.000000e+00  1.323803e+00         NaN   \n",
      "50%    9.900000e+01  4.240000e+02  1.200000e+01  1.763827e+00         NaN   \n",
      "75%    1.360000e+02  6.360000e+02  1.700000e+01  2.393846e+00         NaN   \n",
      "max    1.690000e+02  8.480000e+02  3.800000e+01  6.011999e+00         NaN   \n",
      "\n",
      "       feature_01  feature_02  feature_03  feature_04    feature_05  ...  \\\n",
      "count         0.0         0.0         0.0         0.0  1.944210e+06  ...   \n",
      "mean          NaN         NaN         NaN         NaN -4.463175e-02  ...   \n",
      "std           NaN         NaN         NaN         NaN  9.471079e-01  ...   \n",
      "min           NaN         NaN         NaN         NaN -1.176608e+01  ...   \n",
      "25%           NaN         NaN         NaN         NaN -4.756072e-01  ...   \n",
      "50%           NaN         NaN         NaN         NaN -5.818180e-02  ...   \n",
      "75%           NaN         NaN         NaN         NaN  3.493771e-01  ...   \n",
      "max           NaN         NaN         NaN         NaN  1.532000e+01  ...   \n",
      "\n",
      "         feature_78   responder_0   responder_1   responder_2   responder_3  \\\n",
      "count  1.944210e+06  1.944210e+06  1.944210e+06  1.944210e+06  1.944210e+06   \n",
      "mean  -9.805073e-02  8.424639e-03  1.076465e-02  2.412764e-03  1.139089e-02   \n",
      "std    6.392438e-01  9.543020e-01  1.139575e+00  8.429762e-01  1.274226e+00   \n",
      "min   -3.393299e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00   \n",
      "25%   -3.195696e-01 -2.322211e-01 -2.667868e-01 -1.210319e-01 -4.444537e-01   \n",
      "50%   -2.468792e-01 -3.943805e-03 -2.333469e-02 -1.248489e-03 -1.031224e-02   \n",
      "75%   -1.256560e-01  2.309678e-01  2.538152e-01  1.191232e-01  4.292141e-01   \n",
      "max    4.370195e+01  5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00   \n",
      "\n",
      "        responder_4   responder_5   responder_6   responder_7   responder_8  \n",
      "count  1.944210e+06  1.944210e+06  1.944210e+06  1.944210e+06  1.944210e+06  \n",
      "mean   2.185480e-02  3.326982e-03  1.487634e-03 -4.817980e-04  1.078154e-03  \n",
      "std    1.264991e+00  1.222891e+00  8.696651e-01  8.909110e-01  8.726581e-01  \n",
      "min   -5.000000e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00 -5.000000e+00  \n",
      "25%   -5.146699e-01 -2.569897e-01 -3.558709e-01 -3.914038e-01 -2.892584e-01  \n",
      "50%   -1.836913e-02 -4.690550e-03 -9.597129e-03 -2.376243e-02 -3.107830e-04  \n",
      "75%    5.065366e-01  2.438874e-01  3.360999e-01  3.463979e-01  2.840819e-01  \n",
      "max    5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00  5.000000e+00  \n",
      "\n",
      "[8 rows x 92 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get summary statistics for the dataset\n",
    "summary_stats = sample_partition.describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(summary_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "responder_6    1.000000\n",
      "responder_3    0.449509\n",
      "responder_8    0.439424\n",
      "responder_7    0.434894\n",
      "responder_4    0.234051\n",
      "                 ...   \n",
      "feature_04          NaN\n",
      "feature_21          NaN\n",
      "feature_26          NaN\n",
      "feature_27          NaN\n",
      "feature_31          NaN\n",
      "Name: responder_6, Length: 92, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = sample_partition.corr()\n",
    "\n",
    "# Extract correlations with responder_6\n",
    "correlation_with_responder_6 = correlation_matrix['responder_6'].sort_values(ascending=False)\n",
    "\n",
    "# Display the correlations with responder_6\n",
    "print(correlation_with_responder_6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a metrics directory if it doesn't exist\n",
    "metrics_dir = \"metrics\"\n",
    "if not os.path.exists(metrics_dir):\n",
    "    os.makedirs(metrics_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = sample_partition.isnull().sum()\n",
    "\n",
    "# Filter only columns with missing values\n",
    "missing_values_log = missing_values[missing_values > 0]\n",
    "\n",
    "# Save missing values to a file\n",
    "with open(os.path.join(metrics_dir, \"missing_values.txt\"), \"w\") as f:\n",
    "    f.write(\"Missing Values Report\\n\\n\")\n",
    "    f.write(str(missing_values_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "missing_values = sample_partition.isnull().sum()\n",
    "\n",
    "# Filter only columns with missing values\n",
    "missing_values_log = missing_values[missing_values > 0]\n",
    "\n",
    "# Save missing values to a file\n",
    "with open(os.path.join(metrics_dir, \"missing_values.txt\"), \"w\") as f:\n",
    "    f.write(\"Missing Values Report\\n\\n\")\n",
    "    f.write(str(missing_values_log))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for the dataset\n",
    "summary_stats = sample_partition.describe()\n",
    "\n",
    "# Save summary statistics to a file\n",
    "with open(os.path.join(metrics_dir, \"summary_statistics.txt\"), \"w\") as f:\n",
    "    f.write(\"Summary Statistics\\n\\n\")\n",
    "    f.write(str(summary_stats))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = sample_partition.corr()\n",
    "\n",
    "# Extract correlations with responder_6\n",
    "correlation_with_responder_6 = correlation_matrix['responder_6'].sort_values(ascending=False)\n",
    "\n",
    "# Save correlations with responder_6 to a file\n",
    "with open(os.path.join(metrics_dir, \"correlation_with_responder_6.txt\"), \"w\") as f:\n",
    "    f.write(\"Correlations with responder_6\\n\\n\")\n",
    "    f.write(str(correlation_with_responder_6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after handling missing values: (1944210, 83)\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with the median for moderate missingness\n",
    "sample_partition.fillna(sample_partition.median(), inplace=True)\n",
    "\n",
    "# Drop columns with excessive missing values (more than 80% missing)\n",
    "threshold = 0.8 * len(sample_partition)\n",
    "sample_partition = sample_partition.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Check the shape of the data after handling missing values\n",
    "print(\"Shape after handling missing values:\", sample_partition.shape)\n",
    "\n",
    "# Save the updated dataset shape to metrics\n",
    "with open(os.path.join(metrics_dir, \"updated_dataset_shape.txt\"), \"w\") as f:\n",
    "    f.write(f\"Shape after missing value imputation and column drops: {sample_partition.shape}\\n\")\n",
    "\n",
    "# Check if there are still any missing values\n",
    "remaining_missing = sample_partition.isnull().sum().sum()\n",
    "print(\"Remaining missing values:\", remaining_missing)\n",
    "\n",
    "# Save remaining missing values to the log (if any)\n",
    "with open(os.path.join(metrics_dir, \"remaining_missing_values.txt\"), \"w\") as f:\n",
    "    f.write(f\"Remaining missing values: {remaining_missing}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged features for responder_6 (1 day lag)\n",
    "sample_partition['responder_6_lag_1'] = sample_partition['responder_6'].shift(1)\n",
    "\n",
    "# You can create more lagged versions if necessary\n",
    "# e.g., sample_partition['responder_6_lag_2'] = sample_partition['responder_6'].shift(2)\n",
    "\n",
    "# Drop any rows with NaNs generated from the lagging\n",
    "sample_partition.dropna(inplace=True)\n",
    "\n",
    "# Save updated dataset with lagged features to log\n",
    "with open(os.path.join(metrics_dir, \"lagged_features_log.txt\"), \"w\") as f:\n",
    "    f.write(\"Created lagged features for responder_6 (1-day lag)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inefficient runtime--refer the .py file instead\n",
    "\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import r2_score\n",
    "\n",
    "# # Split the data into features (X) and target (y)\n",
    "# X = sample_partition.drop(columns=['responder_6'])  # Features\n",
    "# y = sample_partition['responder_6']                # Target\n",
    "\n",
    "# # Split the dataset into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Initialize and train a Random Forest Regressor\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# rf_model.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on the test set\n",
    "# y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# # Calculate R² score\n",
    "# r2_score_value = r2_score(y_test, y_pred)\n",
    "# print(\"R² score:\", r2_score_value)\n",
    "\n",
    "# # Save R² score to metrics\n",
    "# with open(os.path.join(metrics_dir, \"random_forest_r2_score.txt\"), \"w\") as f:\n",
    "#     f.write(f\"Random Forest R² score: {r2_score_value}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def weighted_r2_score(y_true, y_pred, sample_weights):\n",
    "    # Calculate weighted squared error (numerator)\n",
    "    weighted_squared_error = np.sum(sample_weights * (y_true - y_pred) ** 2)\n",
    "    \n",
    "    # Calculate weighted total sum of squares (denominator)\n",
    "    weighted_total_sum_of_squares = np.sum(sample_weights * (y_true ** 2))\n",
    "    \n",
    "    # Calculate weighted R² score\n",
    "    weighted_r2 = 1 - (weighted_squared_error / weighted_total_sum_of_squares)\n",
    "    return weighted_r2\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have 'y_test' (true values), 'y_pred' (predicted values), and 'sample_weights' (weight column)\n",
    "sample_weights = X_test['weight']  # Use the weights from your test set\n",
    "\n",
    "# Calculate weighted R²\n",
    "weighted_r2 = weighted_r2_score(y_test, y_pred, sample_weights)\n",
    "print(\"Weighted R² score:\", weighted_r2)\n",
    "\n",
    "# Save weighted R² to metrics\n",
    "with open(os.path.join(metrics_dir, \"weighted_r2_score.txt\"), \"w\") as f:\n",
    "    f.write(f\"Weighted R² score: {weighted_r2}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
